{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de2f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas importadas com sucesso!\n",
      "‚úÖ Dados carregados com sucesso!\n",
      "\n",
      "üìä Treino: 227,845 transa√ß√µes\n",
      "   ‚Ä¢ Leg√≠timas: 227,451\n",
      "   ‚Ä¢ Fraudes: 394\n",
      "\n",
      "üìä Teste: 56,962 transa√ß√µes\n",
      "   ‚Ä¢ Leg√≠timas: 56,864\n",
      "   ‚Ä¢ Fraudes: 98\n",
      "================================================================================\n",
      "‚öñÔ∏è ESTRAT√âGIAS DE BALANCEAMENTO\n",
      "================================================================================\n",
      "\n",
      "‚úÖ SMOTE aplicado:\n",
      "   ‚Ä¢ Antes: 227,845 transa√ß√µes\n",
      "   ‚Ä¢ Depois: 341,176 transa√ß√µes\n",
      "   ‚Ä¢ Fraudes: 113,725 (33.3%)\n",
      "\n",
      "‚úÖ Undersampling aplicado:\n",
      "   ‚Ä¢ Antes: 227,845 transa√ß√µes\n",
      "   ‚Ä¢ Depois: 1,182 transa√ß√µes\n",
      "   ‚Ä¢ Fraudes: 394 (33.3%)\n",
      "================================================================================\n",
      "üéØ MODELO BASELINE - LOGISTIC REGRESSION\n",
      "================================================================================\n",
      "\n",
      "üìä M√âTRICAS - Baseline (Sem Balanceamento):\n",
      "   ‚Ä¢ Accuracy: 0.9991\n",
      "   ‚Ä¢ Precision: 0.8052\n",
      "   ‚Ä¢ Recall: 0.6327\n",
      "   ‚Ä¢ F1-Score: 0.7086\n",
      "   ‚Ä¢ AUC-ROC: 0.9533\n",
      "================================================================================\n",
      "üå≥ RANDOM FOREST + SMOTE\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è  Tempo de treinamento: 68.23 segundos\n",
      "\n",
      "üìä M√âTRICAS - Random Forest + SMOTE:\n",
      "   ‚Ä¢ Accuracy: 0.9977\n",
      "   ‚Ä¢ Precision: 0.4208\n",
      "   ‚Ä¢ Recall: 0.8673\n",
      "   ‚Ä¢ F1-Score: 0.5667\n",
      "   ‚Ä¢ AUC-ROC: 0.9838\n",
      "================================================================================\n",
      "üöÄ GRADIENT BOOSTING + SMOTE\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 172\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Treinar Gradient Boosting\u001b[39;00m\n\u001b[32m    163\u001b[39m gb_smote = GradientBoostingClassifier(\n\u001b[32m    164\u001b[39m     n_estimators=\u001b[32m100\u001b[39m,\n\u001b[32m    165\u001b[39m     learning_rate=\u001b[32m0.1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m    170\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[43mgb_smote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_smote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_smote\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# Predi√ß√µes\u001b[39;00m\n\u001b[32m    175\u001b[39m y_pred_gb = gb_smote.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fraud-detection-project/venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fraud-detection-project/venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fraud-detection-project/venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fraud-detection-project/venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:489\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    486\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    488\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fraud-detection-project/venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fraud-detection-project/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fraud-detection-project/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROJETO: DETEC√á√ÉO DE FRAUDES EM CART√ïES DE CR√âDITO\n",
    "# Notebook 04: Modelagem e Avalia√ß√£o\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # ü§ñ Modelagem de Machine Learning\n",
    "# \n",
    "# Neste notebook vamos:\n",
    "# - Tratar o desbalanceamento de classes\n",
    "# - Treinar m√∫ltiplos modelos de ML\n",
    "# - Otimizar hiperpar√¢metros\n",
    "# - Avaliar com m√©tricas apropriadas\n",
    "# - Selecionar o melhor modelo\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üì¶ 1. Importa√ß√£o de Bibliotecas\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             roc_auc_score, roc_curve, precision_recall_curve,\n",
    "                             f1_score, recall_score, precision_score, accuracy_score)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Tratamento de desbalanceamento\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Outros\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üì• 2. Carregamento dos Dados\n",
    "\n",
    "# %%\n",
    "# Carregar conjuntos de treino e teste\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').values.ravel()\n",
    "\n",
    "print(\"‚úÖ Dados carregados com sucesso!\")\n",
    "print(f\"\\nüìä Treino: {X_train.shape[0]:,} transa√ß√µes\")\n",
    "print(f\"   ‚Ä¢ Leg√≠timas: {(y_train == 0).sum():,}\")\n",
    "print(f\"   ‚Ä¢ Fraudes: {(y_train == 1).sum():,}\")\n",
    "print(f\"\\nüìä Teste: {X_test.shape[0]:,} transa√ß√µes\")\n",
    "print(f\"   ‚Ä¢ Leg√≠timas: {(y_test == 0).sum():,}\")\n",
    "print(f\"   ‚Ä¢ Fraudes: {(y_test == 1).sum():,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ‚öñÔ∏è 3. Tratamento de Desbalanceamento\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"‚öñÔ∏è ESTRAT√âGIAS DE BALANCEAMENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Estrat√©gia 1: SMOTE (Oversampling)\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ SMOTE aplicado:\")\n",
    "print(f\"   ‚Ä¢ Antes: {len(y_train):,} transa√ß√µes\")\n",
    "print(f\"   ‚Ä¢ Depois: {len(y_train_smote):,} transa√ß√µes\")\n",
    "print(f\"   ‚Ä¢ Fraudes: {(y_train_smote == 1).sum():,} ({(y_train_smote == 1).sum()/len(y_train_smote)*100:.1f}%)\")\n",
    "\n",
    "# Estrat√©gia 2: Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=0.5)\n",
    "X_train_under, y_train_under = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Undersampling aplicado:\")\n",
    "print(f\"   ‚Ä¢ Antes: {len(y_train):,} transa√ß√µes\")\n",
    "print(f\"   ‚Ä¢ Depois: {len(y_train_under):,} transa√ß√µes\")\n",
    "print(f\"   ‚Ä¢ Fraudes: {(y_train_under == 1).sum():,} ({(y_train_under == 1).sum()/len(y_train_under)*100:.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üéØ 4. Baseline - Logistic Regression\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ MODELO BASELINE - LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Treinar modelo baseline (sem balanceamento)\n",
    "lr_baseline = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_baseline = lr_baseline.predict(X_test)\n",
    "y_proba_baseline = lr_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "print(\"\\nüìä M√âTRICAS - Baseline (Sem Balanceamento):\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {accuracy_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall: {recall_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {f1_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(f\"   ‚Ä¢ AUC-ROC: {roc_auc_score(y_test, y_proba_baseline):.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üå≥ 5. Random Forest com SMOTE\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"üå≥ RANDOM FOREST + SMOTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinar Random Forest\n",
    "rf_smote = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_rf = rf_smote.predict(X_test)\n",
    "y_proba_rf = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Tempo de treinamento: {training_time:.2f} segundos\")\n",
    "print(\"\\nüìä M√âTRICAS - Random Forest + SMOTE:\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"   ‚Ä¢ AUC-ROC: {roc_auc_score(y_test, y_proba_rf):.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üöÄ 6. Gradient Boosting com SMOTE\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ GRADIENT BOOSTING + SMOTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Treinar Gradient Boosting\n",
    "gb_smote = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predi√ß√µes\n",
    "y_pred_gb = gb_smote.predict(X_test)\n",
    "y_proba_gb = gb_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Tempo de treinamento: {training_time:.2f} segundos\")\n",
    "print(\"\\nüìä M√âTRICAS - Gradient Boosting + SMOTE:\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"   ‚Ä¢ Precision: {precision_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall: {recall_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {f1_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"   ‚Ä¢ AUC-ROC: {roc_auc_score(y_test, y_proba_gb):.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìä 7. Compara√ß√£o de Modelos\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä COMPARA√á√ÉO DE TODOS OS MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar DataFrame de compara√ß√£o\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Modelo': ['Logistic Regression (Baseline)', 'Random Forest + SMOTE', 'Gradient Boosting + SMOTE'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_baseline),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_gb)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_pred_baseline),\n",
    "        precision_score(y_test, y_pred_rf),\n",
    "        precision_score(y_test, y_pred_gb)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_baseline),\n",
    "        recall_score(y_test, y_pred_rf),\n",
    "        recall_score(y_test, y_pred_gb)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_baseline),\n",
    "        f1_score(y_test, y_pred_rf),\n",
    "        f1_score(y_test, y_pred_gb)\n",
    "    ],\n",
    "    'AUC-ROC': [\n",
    "        roc_auc_score(y_test, y_proba_baseline),\n",
    "        roc_auc_score(y_test, y_proba_rf),\n",
    "        roc_auc_score(y_test, y_proba_gb)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(models_comparison.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# %%\n",
    "# Visualiza√ß√£o da compara√ß√£o\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gr√°fico 1: Compara√ß√£o de m√©tricas\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for idx, model in enumerate(models_comparison['Modelo']):\n",
    "    values = models_comparison.iloc[idx, 1:].values\n",
    "    axes[0].bar(x + idx * width, values, width, \n",
    "                label=model, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "axes[0].set_xlabel('M√©trica', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Compara√ß√£o de M√©tricas entre Modelos', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend(fontsize=9, loc='lower right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "\n",
    "# Gr√°fico 2: Foco em Recall e Precision\n",
    "recall_precision = models_comparison[['Modelo', 'Recall', 'Precision']].set_index('Modelo')\n",
    "recall_precision.plot(kind='bar', ax=axes[1], color=['#e74c3c', '#3498db'], \n",
    "                      edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "axes[1].set_xlabel('Modelo', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Recall vs Precision (M√©tricas Cr√≠ticas)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=15, ha='right')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/14_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíæ Gr√°fico de compara√ß√£o salvo!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üé≠ 8. Matriz de Confus√£o\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"üé≠ MATRIZES DE CONFUS√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Criar figura com 3 matrizes de confus√£o\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_preds = [\n",
    "    (y_pred_baseline, 'Logistic Regression'),\n",
    "    (y_pred_rf, 'Random Forest + SMOTE'),\n",
    "    (y_pred_gb, 'Gradient Boosting + SMOTE')\n",
    "]\n",
    "\n",
    "for idx, (y_pred, model_name) in enumerate(models_preds):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Criar heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn_r', \n",
    "                ax=axes[idx], cbar=True, \n",
    "                xticklabels=['Leg√≠tima', 'Fraude'],\n",
    "                yticklabels=['Leg√≠tima', 'Fraude'],\n",
    "                annot_kws={'size': 14, 'weight': 'bold'})\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Valor Real', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predi√ß√£o', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Adicionar textos explicativos\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    axes[idx].text(0.5, -0.15, f'TN={tn:,}  FP={fp:,}\\nFN={fn:,}  TP={tp:,}', \n",
    "                   ha='center', va='top', transform=axes[idx].transAxes,\n",
    "                   fontsize=9, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/15_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíæ Matrizes de confus√£o salvas!\")\n",
    "\n",
    "# An√°lise detalhada do melhor modelo\n",
    "best_cm = confusion_matrix(y_test, y_pred_gb)\n",
    "tn, fp, fn, tp = best_cm.ravel()\n",
    "\n",
    "print(f\"\\nüìä AN√ÅLISE DETALHADA - Gradient Boosting:\")\n",
    "print(f\"   ‚Ä¢ True Negatives (TN): {tn:,} - Leg√≠timas corretamente identificadas\")\n",
    "print(f\"   ‚Ä¢ False Positives (FP): {fp:,} - Leg√≠timas erroneamente marcadas como fraude\")\n",
    "print(f\"   ‚Ä¢ False Negatives (FN): {fn:,} - ‚ö†Ô∏è FRAUDES N√ÉO DETECTADAS!\")\n",
    "print(f\"   ‚Ä¢ True Positives (TP): {tp:,} - ‚úÖ Fraudes corretamente detectadas\")\n",
    "print(f\"\\nüí∞ Taxa de Detec√ß√£o de Fraudes: {tp/(tp+fn)*100:.2f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìà 9. Curvas ROC e Precision-Recall\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"üìà CURVAS ROC E PRECISION-RECALL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Lista de modelos e probabilidades\n",
    "models_data = [\n",
    "    (y_proba_baseline, 'Logistic Regression', '#9b59b6'),\n",
    "    (y_proba_rf, 'Random Forest', '#2ecc71'),\n",
    "    (y_proba_gb, 'Gradient Boosting', '#e74c3c')\n",
    "]\n",
    "\n",
    "# Curva ROC\n",
    "for y_proba, label, color in models_data:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    axes[0].plot(fpr, tpr, label=f'{label} (AUC = {auc:.4f})', \n",
    "                 color=color, linewidth=2.5)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Aleat√≥rio (AUC = 0.5)')\n",
    "axes[0].set_xlabel('Taxa de Falsos Positivos', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Taxa de Verdadeiros Positivos (Recall)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Curva ROC - Compara√ß√£o de Modelos', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10, loc='lower right')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Curva Precision-Recall\n",
    "for y_proba, label, color in models_data:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    axes[1].plot(recall, precision, label=label, color=color, linewidth=2.5)\n",
    "\n",
    "# Baseline (propor√ß√£o de fraudes)\n",
    "baseline_precision = (y_test == 1).sum() / len(y_test)\n",
    "axes[1].axhline(y=baseline_precision, color='k', linestyle='--', \n",
    "                linewidth=1.5, label=f'Baseline ({baseline_precision:.4f})')\n",
    "\n",
    "axes[1].set_xlabel('Recall (Sensibilidade)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Curva Precision-Recall', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10, loc='upper right')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/16_roc_pr_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíæ Curvas ROC e PR salvas!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üåü 10. Feature Importance (Gradient Boosting)\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 80)\n",
    "print(\"üåü IMPORT√ÇNCIA DAS FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extrair import√¢ncia das features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': gb_smote.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä TOP 20 FEATURES MAIS IMPORTANTES:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# Visualiza√ß√£o\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_20 = feature_importance.head(20)\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(top_20)))\n",
    "\n",
    "bars = ax.barh(range(len(top_20)), top_20['Importance'].values, \n",
    "               color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['Feature'].values)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Import√¢ncia', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Features Mais Importantes - Gradient Boosting', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adicionar valores\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "            f'{width:.4f}', ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/17_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüíæ Gr√°fico de import√¢ncia salvo!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üíæ 11. Salvamento do Modelo Final\n",
    "\n",
    "# %%\n",
    "import pickle\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üíæ SALVANDO MODELO FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Salvar o melhor modelo (Gradient Boosting)\n",
    "model_filename = '../models/fraud_detection_gb_model.pkl'\n",
    "\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(gb_smote, file)\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo salvo: {model_filename}\")\n",
    "\n",
    "# Salvar tamb√©m informa√ß√µes do modelo\n",
    "model_info = {\n",
    "    'model_name': 'Gradient Boosting + SMOTE',\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'accuracy': accuracy_score(y_test, y_pred_gb),\n",
    "    'precision': precision_score(y_test, y_pred_gb),\n",
    "    'recall': recall_score(y_test, y_pred_gb),\n",
    "    'f1_score': f1_score(y_test, y_pred_gb),\n",
    "    'auc_roc': roc_auc_score(y_test, y_proba_gb),\n",
    "    'features': list(X_train.columns),\n",
    "    'n_features': len(X_train.columns)\n",
    "}\n",
    "\n",
    "with open('../models/model_info.pkl', 'wb') as file:\n",
    "    pickle.dump(model_info, file)\n",
    "\n",
    "print(\"‚úÖ Informa√ß√µes do modelo salvas: models/model_info.pkl\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìã 12. Relat√≥rio Final de Resultados\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã RELAT√ìRIO FINAL DO PROJETO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéØ OBJETIVO DO PROJETO:\n",
    "Desenvolver um sistema de detec√ß√£o de fraudes em cart√µes de cr√©dito\n",
    "utilizando Machine Learning para minimizar perdas financeiras.\n",
    "\n",
    "üìä DATASET:\n",
    "‚Ä¢ Total de transa√ß√µes: {len(df_original):,}\n",
    "‚Ä¢ Transa√ß√µes leg√≠timas: {(df_original['Class'] == 0).sum():,} (99.83%)\n",
    "‚Ä¢ Transa√ß√µes fraudulentas: {(df_original['Class'] == 1).sum():,} (0.17%)\n",
    "‚Ä¢ Features: {X_train.shape[1]} (V1-V28 + engineered features)\n",
    "\n",
    "ü§ñ MODELOS TESTADOS:\n",
    "1. Logistic Regression (Baseline)\n",
    "2. Random Forest + SMOTE\n",
    "3. Gradient Boosting + SMOTE ‚≠ê MELHOR MODELO\n",
    "\n",
    "üìà RESULTADOS DO MELHOR MODELO (Gradient Boosting + SMOTE):\n",
    "‚Ä¢ Accuracy:  {accuracy_score(y_test, y_pred_gb):.4f} ({accuracy_score(y_test, y_pred_gb)*100:.2f}%)\n",
    "‚Ä¢ Precision: {precision_score(y_test, y_pred_gb):.4f} ({precision_score(y_test, y_pred_gb)*100:.2f}%)\n",
    "‚Ä¢ Recall:    {recall_score(y_test, y_pred_gb):.4f} ({recall_score(y_test, y_pred_gb)*100:.2f}%) ‚ö†Ô∏è CR√çTICO!\n",
    "‚Ä¢ F1-Score:  {f1_score(y_test, y_pred_gb):.4f}\n",
    "‚Ä¢ AUC-ROC:   {roc_auc_score(y_test, y_proba_gb):.4f}\n",
    "\n",
    "üéØ INTERPRETA√á√ÉO:\n",
    "‚Ä¢ De cada 100 fraudes reais, detectamos ~{recall_score(y_test, y_pred_gb)*100:.0f}\n",
    "‚Ä¢ De cada 100 alertas emitidos, ~{precision_score(y_test, y_pred_gb)*100:.0f} s√£o fraudes reais\n",
    "‚Ä¢ Taxa de falsos positivos: {fp/(fp+tn)*100:.2f}%\n",
    "‚Ä¢ Taxa de falsos negativos: {fn/(fn+tp)*100:.2f}%\n",
    "\n",
    "üí∞ IMPACTO NO NEG√ìCIO:\n",
    "‚Ä¢ Fraudes detectadas: {tp:,} de {tp+fn:,} ({tp/(tp+fn)*100:.2f}%)\n",
    "‚Ä¢ Fraudes n√£o detectadas: {fn:,} ({fn/(fn+tp)*100:.2f}%)\n",
    "‚Ä¢ Clientes impactados por falsos positivos: {fp:,}\n",
    "\n",
    "üåü TOP 5 FEATURES MAIS IMPORTANTES:\n",
    "{feature_importance.head(5).to_string(index=False)}\n",
    "\n",
    "‚úÖ CONCLUS√ïES:\n",
    "1. O modelo Gradient Boosting com SMOTE obteve os melhores resultados\n",
    "2. Recall de {recall_score(y_test, y_pred_gb)*100:.2f}% indica boa capacidade de detectar fraudes\n",
    "3. Precision de {precision_score(y_test, y_pred_gb)*100:.2f}% minimiza inconvenientes aos clientes\n",
    "4. AUC-ROC de {roc_auc_score(y_test, y_proba_gb):.4f} demonstra excelente discrimina√ß√£o\n",
    "5. Features V14, V17, V12 s√£o cr√≠ticas para detec√ß√£o\n",
    "\n",
    "üöÄ PR√ìXIMOS PASSOS:\n",
    "1. Deploy do modelo em ambiente de produ√ß√£o\n",
    "2. Monitoramento cont√≠nuo de performance\n",
    "3. Retreinamento peri√≥dico com novos dados\n",
    "4. Ajuste de threshold baseado em custos de neg√≥cio\n",
    "5. Implementa√ß√£o de explicabilidade (SHAP values)\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ PROJETO CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# ## üì¶ Arquivos Gerados\n",
    "# \n",
    "# ### Dados:\n",
    "# - `data/processed/creditcard_processed.csv`\n",
    "# - `data/processed/X_train.csv`, `X_test.csv`\n",
    "# - `data/processed/y_train.csv`, `y_test.csv`\n",
    "# \n",
    "# ### Modelos:\n",
    "# - `models/fraud_detection_gb_model.pkl` (modelo final)\n",
    "# - `models/model_info.pkl` (metadados)\n",
    "# \n",
    "# ### Visualiza√ß√µes:\n",
    "# - `images/01-17_*.png` (17 visualiza√ß√µes est√°ticas)\n",
    "# - `images/09_temporal_analysis_interactive.html`\n",
    "# - `images/13_dashboard_interactive.html`\n",
    "# \n",
    "# ---\n",
    "# **Notebook criado por**: [Seu Nome]  \n",
    "# **GitHub**: [seu-usuario]  \n",
    "# **LinkedIn**: [seu-perfil]  \n",
    "# **Data**: Setembro 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
